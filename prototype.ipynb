{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import\n",
    "import imageio\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model_all import VideoDataSet, HNeRV, HNeRVDecoder, TransformInput\n",
    "from hnerv_utils import *\n",
    "from torch.utils.data import Subset\n",
    "from copy import deepcopy \n",
    "from dahuffman import HuffmanCodec\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import decord\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace argparse from default HNeRV code\n",
    "class Argparse():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 vid,\n",
    "                 checkpoint_pth,\n",
    "                shuffle_data = True ,\n",
    "                finetune_ep = [10,20,30,40,50],\n",
    "                base_prune_ratio = 0.60,\n",
    "                prune_ratio_step = 0.10,\n",
    "                data_split = '1_1_1' ,\n",
    "                crop_list = '640_1280' ,\n",
    "                resize_list = '-1',\n",
    "                embed =  '' ,\n",
    "                ks = '0_3_3' ,\n",
    "                enc_strds = [] ,\n",
    "                enc_dim = '64_16' ,\n",
    "                modelsize = 1.5 ,\n",
    "                saturate_stages = -1 ,\n",
    "                fc_hw = '9_16' ,\n",
    "                reduce = 1.2 ,\n",
    "                lower_width = 32 ,\n",
    "                dec_strds = [5, 3, 2, 2, 2] ,\n",
    "                num_blks = '1_1' ,\n",
    "                conv_type = ['convnext', 'pshuffel'],\n",
    "                norm = 'none' ,\n",
    "                act = 'gelu' ,\n",
    "                workers = 4, \n",
    "                batchSize = 1 ,\n",
    "                start_epoch = -1 ,\n",
    "                not_resume = True ,\n",
    "                epochs = 5 ,\n",
    "                block_params = '1_1' ,\n",
    "                lr = '0.001' ,\n",
    "                lr_type = 'cosine_0.1_1_0.1' ,\n",
    "                loss = 'Fusion6' ,\n",
    "                out_bias = 'tanh' ,\n",
    "                eval_only = False ,\n",
    "                eval_freq = '10' ,\n",
    "                quant_model_bit = 8 ,\n",
    "                quant_embed_bit = 6 ,\n",
    "                quant_axis = '0.0' ,\n",
    "                dump_images = False ,\n",
    "                dump_videos = False ,\n",
    "                eval_fps = False ,\n",
    "                encoder_file = '' ,\n",
    "                manualSeed = 1337 ,\n",
    "                distributed = False ,\n",
    "                debug = False ,\n",
    "                print_freq = 50 ,\n",
    "                weight = 'None' ,\n",
    "                overwrite = True ,\n",
    "                outf = 'unify' ,\n",
    "                super = False,\n",
    "                super_rate = 2,\n",
    "                suffix = ''):\n",
    "\n",
    "        #Set param\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_pth = checkpoint_pth\n",
    "        self.vid = vid\n",
    "        self.shuffle_data  = shuffle_data\n",
    "        self.data_split = data_split\n",
    "        self.crop_list = crop_list\n",
    "        self.resize_list = resize_list\n",
    "        self.embed = embed\n",
    "        self.ks = ks\n",
    "        self.enc_strds = enc_strds\n",
    "        self.enc_dim = enc_dim\n",
    "        self.modelsize = modelsize\n",
    "        self.saturate_stages = saturate_stages\n",
    "        self.fc_hw = fc_hw\n",
    "        self.reduce = reduce\n",
    "        self.lower_width = lower_width\n",
    "        self.dec_strds = dec_strds\n",
    "        self.num_blks = num_blks\n",
    "        self.conv_type = conv_type\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.workers = workers\n",
    "        self.batchSize = batchSize\n",
    "        self.start_epoch = start_epoch\n",
    "        self.not_resume = not_resume\n",
    "        self.epochs = epochs\n",
    "        self.block_params = block_params\n",
    "        self.lr = lr\n",
    "        self.lr_type = lr_type\n",
    "        self.loss = loss\n",
    "        self.out_bias = out_bias\n",
    "        self.eval_only = eval_only\n",
    "        self.eval_freq = eval_freq\n",
    "        self.quant_model_bit = quant_model_bit\n",
    "        self.quant_embed_bit = quant_embed_bit\n",
    "        self.quant_axis = quant_axis\n",
    "        self.dump_images = dump_images\n",
    "        self.dump_videos = dump_videos\n",
    "        self.eval_fps = eval_fps\n",
    "        self.encoder_file = encoder_file\n",
    "        self.manualSeed = manualSeed\n",
    "        self.distributed = distributed\n",
    "        self.debug = debug\n",
    "        self.print_freq = print_freq\n",
    "        self.weight = weight\n",
    "        self.overwrite = overwrite\n",
    "        self.outf = outf\n",
    "        self.suffix = suffix      \n",
    "        self.compression_method = 'huffman'\n",
    "        self.super = super\n",
    "        self.super_rate = super_rate\n",
    "        self.eval_quant_overall_PSNR = 0\n",
    "        self.eval_orig_overall_PSNR = 0\n",
    "        self.finetune_ep = finetune_ep\n",
    "        self.base_prune_ratio = base_prune_ratio\n",
    "        self.prune_ratio_step = prune_ratio_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init\n",
    "torch.set_printoptions(precision=4) \n",
    "\n",
    "args = Argparse(data_path='/scratch/ar7996/ivp/uvgdataset/Beauty_3840x2160_120fps_420_8bit_YUV_RAW/Beauty_3840x2160_YUV_444_1920x1080_cropped_images/',\n",
    "                vid='prune_test_beauty_2_oriquantize_QM6_2x',\n",
    "                checkpoint_pth='/scratch/kl3866/IVP/HNeRV/batches/final/2x_1080_HNeRV_gamut/2x_HNeRV_gamut/2x_Beauty_QTM32_QTE6/1_1_1__Dim64_16_FC9_16_KS0_1_5_RED1.2_low12_blk1_1_e600_b1_quant_M32_E6_lr0.001_cosine_0.1_1_0.1_L2_Size1.5_ENC_convnext_5,4,4,3,2_DEC_pshuffel_5,4,4,3,2_gelu1_1/',\n",
    "                outf='new_prune_test_4_2x',\n",
    "                base_prune_ratio=0.05,\n",
    "                prune_ratio_step=0.05,\n",
    "                super=True,\n",
    "                super_rate=2,\n",
    "                loss='L2',\n",
    "                crop_list='960_1920',\n",
    "                enc_strds=np.array([5,4,4,3,2]),\n",
    "                dec_strds=np.array([5,4,4,3,2]),\n",
    "                enc_dim='64_16',\n",
    "                ks='0_1_5',\n",
    "                reduce=1.2,\n",
    "                eval_freq=5,\n",
    "                lower_width=12,\n",
    "                batchSize=1,\n",
    "                lr=0.001,\n",
    "                eval_only=False,\n",
    "#                 weight=\"checkpoints/hnerv-1.5m-e300.pth\",\n",
    "                dump_images=True,\n",
    "                finetune_ep=[0,10,20,30,\n",
    "                             50,70,90,110,\n",
    "                             140,170,200,230,\n",
    "                             270,310,350,390,\n",
    "                             440,500,570,650], \n",
    "                epochs=651,\n",
    "                quant_model_bit=6\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init\n",
    "torch.set_printoptions(precision=4) \n",
    "\n",
    "args = Argparse(data_path='/scratch/ar7996/ivp/uvgdataset/Beauty_3840x2160_120fps_420_8bit_YUV_RAW/Beauty_3840x2160_YUV_444_1920x1080_cropped_images/',\n",
    "                vid='large_embed_beauty_1_MAXVAL256_2x',\n",
    "                checkpoint_pth='/scratch/kl3866/IVP/HNeRV/batches/final/2x_1080_HNeRV_gamut/2x_HNeRV_gamut/2x_Beauty_QTM32_QTE6/1_1_1__Dim64_16_FC9_16_KS0_1_5_RED1.2_low12_blk1_1_e600_b1_quant_M32_E6_lr0.001_cosine_0.1_1_0.1_L2_Size1.5_ENC_convnext_5,4,4,3,2_DEC_pshuffel_5,4,4,3,2_gelu1_1/',\n",
    "                outf='large_embed_test_0_2x',\n",
    "                base_prune_ratio=0.8,\n",
    "                prune_ratio_step=0.05,\n",
    "                super=True,\n",
    "                super_rate=2,\n",
    "                loss='L2',\n",
    "                crop_list='960_1920',\n",
    "                enc_strds=np.array([5,4,3,2,2]),\n",
    "                dec_strds=np.array([5,4,3,2,2]),\n",
    "                enc_dim='64_16',\n",
    "                ks='0_1_5',\n",
    "                reduce=1.2,\n",
    "                eval_freq=5,\n",
    "                lower_width=12,\n",
    "                batchSize=1,\n",
    "                lr=0.001,\n",
    "                eval_only=False,\n",
    "#                 weight=\"checkpoints/hnerv-1.5m-e300.pth\",\n",
    "                dump_images=True,\n",
    "                finetune_ep = [0,1,2],\n",
    "#                 finetune_ep=[0,20,50, #x\n",
    "#                              100,160], \n",
    "                epochs=3,\n",
    "                quant_model_bit=8\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.set_printoptions(precision=4) \n",
    "    if args.debug:\n",
    "        args.eval_freq = 1\n",
    "        args.outf = 'output/debug'\n",
    "    else:\n",
    "        args.outf = os.path.join('output', args.outf)\n",
    "\n",
    "    args.enc_strd_str, args.dec_strd_str = ','.join([str(x) for x in args.enc_strds]), ','.join([str(x) for x in args.dec_strds])\n",
    "    extra_str = 'Size{}_ENC_{}_{}_DEC_{}_{}_{}{}{}'.format(args.modelsize, args.conv_type[0], args.enc_strd_str, \n",
    "        args.conv_type[1], args.dec_strd_str, '' if args.norm == 'none' else f'_{args.norm}', \n",
    "        '_dist' if args.distributed else '', '_shuffle_data' if args.shuffle_data else '',)\n",
    "    args.quant_str = f'quant_M{args.quant_model_bit}_E{args.quant_embed_bit}'\n",
    "    embed_str = f'{args.embed}_Dim{args.enc_dim}'\n",
    "    exp_id = f'{args.vid}/{args.data_split}_{embed_str}_FC{args.fc_hw}_KS{args.ks}_RED{args.reduce}_low{args.lower_width}_blk{args.num_blks}' + \\\n",
    "            f'_e{args.epochs}_b{args.batchSize}_{args.quant_str}_lr{args.lr}_{args.lr_type}_{args.loss}_{extra_str}{args.act}{args.block_params}{args.suffix}'\n",
    "    args.exp_id = exp_id\n",
    "\n",
    "    args.outf = os.path.join(args.outf, exp_id)\n",
    "    if args.overwrite and os.path.isdir(args.outf):\n",
    "        print('Will overwrite the existing output dir!')\n",
    "#         raise Exception(\"Warning overwriting stuff, break!\")\n",
    "        shutil.rmtree(args.outf)\n",
    "\n",
    "    if not os.path.isdir(args.outf):\n",
    "        os.makedirs(args.outf)\n",
    "\n",
    "    port = hash(args.exp_id) % 20000 + 10000\n",
    "    args.init_method =  f'tcp://127.0.0.1:{port}'\n",
    "    print(f'init_method: {args.init_method}', flush=True)\n",
    "\n",
    "    torch.set_printoptions(precision=2) \n",
    "    args.ngpus_per_node = torch.cuda.device_count()\n",
    "    if args.distributed and args.ngpus_per_node > 1:\n",
    "        mp.spawn(train, nprocs=args.ngpus_per_node, args=(args,))\n",
    "    else:\n",
    "        return train(None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_gpu(x, device):\n",
    "    return x.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### New Train Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New train func\n",
    "def prune(model,ratio=0.05):\n",
    "    \n",
    "    #Count params first\n",
    "    param_ct = 0\n",
    "\n",
    "    for k,v in model.state_dict().items():\n",
    "\n",
    "        #Only count decoder and head layer\n",
    "        if 'decoder' in k or 'head' in k:\n",
    "            params = np.prod(v.shape)\n",
    "            param_ct += params\n",
    "\n",
    "            \n",
    "    #Count zeros (most important)\n",
    "    ori_zero_count = 0\n",
    "\n",
    "    for k,v in model.state_dict().items():\n",
    "\n",
    "        #Only count decoder and head layer\n",
    "        if 'decoder' in k or 'head' in k:\n",
    "            ct = int((v==0).sum())\n",
    "            ori_zero_count += ct\n",
    "            \n",
    "            \n",
    "    #Assemble params to prune\n",
    "    to_prune = []\n",
    "    \n",
    "    #No need to check for weights in head_layer since it's fully convolutional; all trainable and prunable\n",
    "    if model.head_layer != 'Conv2d':\n",
    "        for i in model.head_layer:\n",
    "            to_prune.append((i,'weight'))\n",
    "            to_prune.append((i,'bias'))\n",
    "            \n",
    "    elif model.head_layer == 'Conv2d': #for case without super-res\n",
    "        to_prune.append((model.head_layer,'weight'))\n",
    "        to_prune.append((model.head_layer,'bias'))\n",
    "\n",
    "\n",
    "    #Need to check for decoder\n",
    "    for i in model.decoder:\n",
    "\n",
    "        if 'weight' in dir(i) and 'NeRV' not in i.__repr__():\n",
    "            to_prune.append((i,'weight'))\n",
    "            to_prune.append((i,'bias'))\n",
    "\n",
    "        elif 'NeRV' in i.__repr__():\n",
    "\n",
    "            #Iter through NeRV block\n",
    "            for j in i.modules():\n",
    "                if 'weight' in dir(j): #if with prunable params\n",
    "                    to_prune.append((j,'weight'))\n",
    "                    to_prune.append((j,'bias'))\n",
    "                    \n",
    "                    \n",
    "    #Prune\n",
    "    import torch.nn.utils.prune as prune\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=ratio,\n",
    "    )\n",
    "    \n",
    "    #Remove\n",
    "    for i in to_prune:\n",
    "        prune.remove(*i)\n",
    "        \n",
    "        \n",
    "    #Count zeros (most important)\n",
    "    pruned_zero_count = 0\n",
    "\n",
    "    for k,v in model.state_dict().items():\n",
    "\n",
    "        #Only count decoder and head layer\n",
    "        if 'decoder' in k or 'head' in k:\n",
    "            ct = int((v==0).sum())\n",
    "            pruned_zero_count += ct\n",
    "            \n",
    "    #Compute compression ratio\n",
    "    actual_ratio = pruned_zero_count / param_ct\n",
    "    estimated_rateio = ratio\n",
    "            \n",
    "    return model,pruned_zero_count,estimated_rateio,actual_ratio\n",
    "\n",
    "\n",
    "\n",
    "def train(local_rank, args):\n",
    "    cudnn.benchmark = True\n",
    "    torch.manual_seed(args.manualSeed)\n",
    "    np.random.seed(args.manualSeed)\n",
    "    random.seed(args.manualSeed)\n",
    "\n",
    "    if args.distributed and args.ngpus_per_node > 1:\n",
    "        torch.distributed.init_process_group(\n",
    "            backend='nccl',\n",
    "            init_method=args.init_method,\n",
    "            world_size=args.ngpus_per_node,\n",
    "            rank=local_rank,\n",
    "        )\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        assert torch.distributed.is_initialized()        \n",
    "        args.batchSize = int(args.batchSize / args.ngpus_per_node)\n",
    "\n",
    "    args.metric_names = ['pred_seen_psnr', 'pred_seen_ssim', 'pred_unseen_psnr', 'pred_unseen_ssim',\n",
    "        'quant_seen_psnr', 'quant_seen_ssim', 'quant_unseen_psnr', 'quant_unseen_ssim']\n",
    "    best_metric_list = [torch.tensor(0) for _ in range(len(args.metric_names))]\n",
    "\n",
    "    # setup dataloader    \n",
    "    full_dataset = VideoDataSet(args)\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(full_dataset) if args.distributed else None\n",
    "    full_dataloader = torch.utils.data.DataLoader(full_dataset, batch_size=args.batchSize, shuffle=(sampler is None),\n",
    "            num_workers=args.workers, pin_memory=True, sampler=sampler, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "    args.final_size = full_dataset.final_size\n",
    "    args.full_data_length = len(full_dataset)\n",
    "    split_num_list = [int(x) for x in args.data_split.split('_')]\n",
    "    train_ind_list, args.val_ind_list = data_split(list(range(args.full_data_length)), split_num_list, args.shuffle_data, 0)\n",
    "    args.dump_vis = (args.dump_images or args.dump_videos)\n",
    "\n",
    "    #  Make sure the testing dataset is fixed for every run\n",
    "    train_dataset =  Subset(full_dataset, train_ind_list)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batchSize, shuffle=(train_sampler is None),\n",
    "         num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True, worker_init_fn=worker_init_fn)\n",
    "         \n",
    "    #Re init a new dataloader WITHOUT shuffling for eval\n",
    "    full_dataloader_noshuffle = torch.utils.data.DataLoader(full_dataset, batch_size=args.batchSize, \n",
    "                                                                shuffle=False, num_workers=args.workers,\n",
    "                                                                pin_memory=True, sampler=sampler, \n",
    "                                                                drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # Compute the parameter number\n",
    "    if 'pe' in args.embed or 'le' in args.embed:\n",
    "        embed_param = 0\n",
    "        embed_dim = int(args.embed.split('_')[-1]) * 2\n",
    "        fc_param = np.prod([int(x) for x in args.fc_hw.split('_')])\n",
    "    else:\n",
    "        total_enc_strds = np.prod(args.enc_strds)\n",
    "        embed_hw = args.final_size / total_enc_strds**2\n",
    "        enc_dim1, embed_ratio = [float(x) for x in args.enc_dim.split('_')]\n",
    "        embed_dim = int(embed_ratio * args.modelsize * 1e6 / args.full_data_length / embed_hw) if embed_ratio < 1 else int(embed_ratio) \n",
    "        embed_param = float(embed_dim) / total_enc_strds**2 * args.final_size * args.full_data_length\n",
    "        args.enc_dim = f'{int(enc_dim1)}_{embed_dim}' \n",
    "        fc_param = (np.prod(args.enc_strds) // np.prod(args.dec_strds))**2 * 9\n",
    "\n",
    "    decoder_size = args.modelsize * 1e6 - embed_param\n",
    "    ch_reduce = 1. / args.reduce\n",
    "    dec_ks1, dec_ks2 = [int(x) for x in args.ks.split('_')[1:]]\n",
    "    fix_ch_stages = len(args.dec_strds) if args.saturate_stages == -1 else args.saturate_stages\n",
    "    a =  ch_reduce * sum([ch_reduce**(2*i) * s**2 * min((2*i + dec_ks1), dec_ks2)**2 for i,s in enumerate(args.dec_strds[:fix_ch_stages])])\n",
    "    b =  embed_dim * fc_param \n",
    "    c =  args.lower_width **2 * sum([s**2 * min(2*(fix_ch_stages + i) + dec_ks1, dec_ks2)  **2 for i, s in enumerate(args.dec_strds[fix_ch_stages:])])\n",
    "    args.fc_dim = int(np.roots([a,b,c - decoder_size]).max())\n",
    "\n",
    "    # Building model\n",
    "    model = HNeRV(args)\n",
    "\n",
    "    # Update model here if super-res | HACKY PATCH, integrate formally if works later\n",
    "    if args.super:\n",
    "\n",
    "        #Import \n",
    "        import torch.nn as nn\n",
    "        from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "        #Check loss function\n",
    "        assert args.loss in ['Super-L1','Super-L2','L2','L1'], Exception(\"Error! Super-res enabled but loss function is {}; use Super-L1 or Super-L2 instead.\".format(args.loss))\n",
    "      \n",
    "        #Get original model head_layer's input channel\n",
    "        head_in_channel = model.head_layer.in_channels\n",
    "\n",
    "        #Determine head layer param based on super-res ratio\n",
    "        if args.super_rate == 2:\n",
    "            transpose_stride = 2 \n",
    "            depth_kernel = 2\n",
    "            depth_padding = 1\n",
    "        elif args.super_rate == 4:\n",
    "            transpose_stride = 4 \n",
    "            depth_kernel = 3\n",
    "            depth_padding = 3\n",
    "\n",
    "        model.head_layer = nn.Sequential(nn.ConvTranspose2d(head_in_channel,head_in_channel,3,transpose_stride,1),\n",
    "                                       nn.Conv2d(head_in_channel,head_in_channel, depth_kernel, groups=head_in_channel,padding=depth_padding),#depthwise conv\n",
    "                                       nn.Conv2d(head_in_channel,3, 1,)) #pointwise conv output\n",
    "\n",
    "        #New loss function\n",
    "        criterionLPIPS = LearnedPerceptualImagePatchSimilarity(net_type='squeeze')\n",
    "\n",
    "\n",
    "    ##### get model params and flops #####\n",
    "    if local_rank in [0, None]:\n",
    "        encoder_param = (sum([p.data.nelement() for p in model.encoder.parameters()]) / 1e6) \n",
    "        decoder_param = (sum([p.data.nelement() for p in model.decoder.parameters()]) / 1e6) \n",
    "        total_param = decoder_param + embed_param / 1e6\n",
    "        args.encoder_param, args.decoder_param, args.total_param = encoder_param, decoder_param, total_param\n",
    "        param_str = f'Encoder_{round(encoder_param, 2)}M_Decoder_{round(decoder_param, 2)}M_Total_{round(total_param, 2)}M'\n",
    "        print(f'{args}\\n {model}\\n {param_str}', flush=True)\n",
    "        with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "            f.write(str(model) + '\\n' + f'{param_str}\\n')\n",
    "        writer = SummaryWriter(os.path.join(args.outf, param_str, 'tensorboard'))\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    # distrite model to gpu or parallel\n",
    "    print(\"Use GPU: {} for training\".format(local_rank))\n",
    "    if args.distributed and args.ngpus_per_node > 1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model.to(local_rank), device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False)\n",
    "        \n",
    "        #Do not support distributed training for super-res yet\n",
    "        if args.super:\n",
    "            raise Exception(\"Warning, support for distributed training + Super-Res with perceptual loss is not supported in current code yet\")\n",
    "    \n",
    "    elif args.ngpus_per_node > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "        #Do not support distributed training for super-res yet\n",
    "        if args.super:\n",
    "            raise Exception(\"Warning, support for distributed training + Super-Res with perceptual loss is not supported in current code yet\")\n",
    "    \n",
    "    elif torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "        if args.super:\n",
    "            criterionLPIPS = criterionLPIPS.to('cuda')\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=0.)\n",
    "    args.transform_func = TransformInput(args)\n",
    "\n",
    "\n",
    "    #Load pretrained model to prune\n",
    "    checkpoint_path = os.path.join(args.checkpoint_pth, 'model_latest.pth')\n",
    "    \n",
    "    assert os.path.isfile(checkpoint_path), Exception(\"No pretrained model found!\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> Auto resume loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n",
    "\n",
    "    #Set start epoch at 0 again for finetuning\n",
    "    args.start_epoch = 0\n",
    "    \n",
    "\n",
    "#     if args.start_epoch < 0:\n",
    "#         if checkpoint is not None:\n",
    "#             args.start_epoch = checkpoint['epoch'] \n",
    "#         args.start_epoch = max(args.start_epoch, 0)\n",
    "\n",
    "#     if args.eval_only:\n",
    "\n",
    "#         print_str = 'Evaluation ... \\n {} Results for checkpoint: {}\\n'.format(datetime.now().strftime('%Y_%m_%d_%H_%M_%S'), args.weight)\n",
    "#         results_list, hw = evaluate(model, full_dataloader_noshuffle, local_rank, args, args.dump_vis, args.compression_method)\n",
    "#         print_str = f'PSNR for output {hw} for quant {args.quant_str}: '\n",
    "#         for i, (metric_name, best_metric_value, metric_value) in enumerate(zip(args.metric_names, best_metric_list, results_list)):\n",
    "#             best_metric_value = best_metric_value if best_metric_value > metric_value.max() else metric_value.max()\n",
    "#             cur_v = RoundTensor(best_metric_value, 2 if 'psnr' in metric_name else 4)\n",
    "#             print_str += f'best_{metric_name}: {cur_v} | '\n",
    "#             best_metric_list[i] = best_metric_value\n",
    "#         if local_rank in [0, None]:\n",
    "#             print(print_str, flush=True)\n",
    "#             with open('{}/eval.txt'.format(args.outf), 'a') as f:\n",
    "#                 f.write(print_str + '\\n\\n')        \n",
    "#             args.train_time, args.cur_epoch = 0, args.epochs\n",
    "#             Dump2CSV(args, best_metric_list, results_list, [torch.tensor(0)], 'eval.csv')\n",
    "\n",
    "#         return\n",
    "\n",
    "\n",
    "    # Training\n",
    "    start = datetime.now()\n",
    "\n",
    "    #Init vars\n",
    "    psnr_list = []\n",
    "    args.finetune_ep = np.array(args.finetune_ep)\n",
    "    \n",
    "    #Vars to track time\n",
    "    raw_fp_encode_time = 0\n",
    "    raw_fu_encode_time = 0\n",
    "    \n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        model.train()       \n",
    "        epoch_start_time = datetime.now()\n",
    "        pred_psnr_list = []\n",
    "        \n",
    "        # iterate over dataloader\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        #Prune model\n",
    "        if epoch in args.finetune_ep:\n",
    "            index = (args.finetune_ep==epoch).argmax()\n",
    "            ratio = np.clip(args.base_prune_ratio + (((args.prune_ratio_step) * index)),0,1)            \n",
    "#             print('>>',index,(args.finetune_ep==epoch).argmax(),ratio)\n",
    "            \n",
    "            model,pruned_zero_count,estimated_ratio,actual_ratio = prune(model,ratio=ratio)\n",
    "            \n",
    "                    \n",
    "            print(\"====\"*30)\n",
    "            print(\"Ratio Input: {}\".format(ratio))\n",
    "            print(\"Prune Ratio (Est): {} | Prune Ratio (Act): {} | #Zeros: {}\".format(estimated_ratio,actual_ratio,pruned_zero_count))\n",
    "            print(\"Ep {} of [{}]\".format(epoch,args.finetune_ep))\n",
    "            \n",
    "            \n",
    "        #Store info\n",
    "        args.estimated_prune_ratio = estimated_ratio \n",
    "        args.actual_prune_ratio = actual_ratio\n",
    "        args.pruned_zero_count = pruned_zero_count\n",
    "        \n",
    "        print((epoch + 1), args.finetune_ep, (epoch + 1) in args.finetune_ep)\n",
    "        \n",
    "        for i, sample in enumerate(train_dataloader):\n",
    "            \n",
    "            #Start Track time (full)\n",
    "            start_time_fu = time.time()\n",
    "            \n",
    "            img_data, img_down_data, norm_idx, img_idx = data_to_gpu(sample['img'], device), data_to_gpu(sample['img_down'], device),data_to_gpu(sample['norm_idx'], device), data_to_gpu(sample['idx'], device)\n",
    "            if i > 10 and args.debug:\n",
    "                break\n",
    "                \n",
    "            #Overwrite for super-res\n",
    "            if args.super:\n",
    "                img_gt = img_data\n",
    "                img_data = img_down_data\n",
    "            else: \n",
    "                img_gt = img_data\n",
    "            \n",
    "            cur_input = norm_idx if 'pe' in args.embed else img_data\n",
    "            cur_epoch = (epoch + float(i) / len(train_dataloader)) / args.epochs\n",
    "            lr = adjust_lr(optimizer, cur_epoch, args)\n",
    "            img_out, _, _, encode_time = model(cur_input)\n",
    "            \n",
    "            #End track time (full)\n",
    "            end_time_fu = time.time()\n",
    "            delta_fu = end_time_fu - start_time_fu\n",
    "            delta_fp = encode_time\n",
    "            \n",
    "            raw_fp_encode_time += delta_fp\n",
    "            raw_fu_encode_time += delta_fu\n",
    "            \n",
    "            \n",
    "            final_loss = loss_fn(img_out, img_gt, args.loss,\n",
    "                                 LPIPS = criterionLPIPS if args.super else None)      \n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_psnr_list.append(psnr_fn_single(img_out.detach(), img_gt)) \n",
    "            if i % args.print_freq == 0 or i == len(train_dataloader) - 1:\n",
    "                pred_psnr = torch.cat(pred_psnr_list).mean()\n",
    "                print_str = '[{}] Rank:{}, Epoch[{}/{}], Step [{}/{}], lr:{:.2e} pred_PSNR: {}'.format(\n",
    "                    datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"), local_rank, epoch+1, args.epochs, i+1, len(train_dataloader), lr, \n",
    "                    RoundTensor(pred_psnr, 2))\n",
    "                print(print_str, flush=True)\n",
    "                if local_rank in [0, None]:\n",
    "                    with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "                        f.write(print_str + '\\n')\n",
    "\n",
    "        # collect numbers from other gpus\n",
    "        if args.distributed and args.ngpus_per_node > 1:\n",
    "            pred_psnr = all_reduce([pred_psnr.to(local_rank)])\n",
    "\n",
    "        # ADD train_PSNR TO TENSORBOARD\n",
    "        if local_rank in [0, None]:\n",
    "            h, w = img_out.shape[-2:]\n",
    "            writer.add_scalar(f'Train/pred_PSNR_{h}X{w}', pred_psnr, epoch+1)\n",
    "            writer.add_scalar('Train/lr', lr, epoch+1)\n",
    "            epoch_end_time = datetime.now()\n",
    "            print(\"Time/epoch: \\tCurrent:{:.2f} \\tAverage:{:.2f}\".format( (epoch_end_time - epoch_start_time).total_seconds(), \\\n",
    "                    (epoch_end_time - start).total_seconds() / (epoch + 1 - args.start_epoch) ))\n",
    "\n",
    "        # evaluation\n",
    "        if (epoch + 1) in args.finetune_ep:\n",
    "            results_list, hw = evaluate(model, \n",
    "                                        full_dataloader_noshuffle, \n",
    "                                        local_rank, \n",
    "                                        args, \n",
    "                                        args.dump_vis if epoch == args.epochs - 1 else False, \n",
    "                                        args.compression_method,\n",
    "                                        epoch,\n",
    "                                        ratio)    \n",
    "            \n",
    "#             return results_list,hw\n",
    "                            \n",
    "                                        \n",
    "            if local_rank in [0, None]:\n",
    "                # ADD val_PSNR TO TENSORBOARD\n",
    "                print_str = f'Eval at epoch {epoch+1} for {hw}: '\n",
    "                for i, (metric_name, best_metric_value, metric_value) in enumerate(zip(args.metric_names, best_metric_list, results_list)):\n",
    "                    best_metric_value = best_metric_value if best_metric_value > metric_value.max() else metric_value.max()\n",
    "                    if 'psnr' in metric_name:\n",
    "                        writer.add_scalar(f'Val/{metric_name}_{hw}', metric_value.max(), epoch+1)\n",
    "                        writer.add_scalar(f'Val/best_{metric_name}_{hw}', best_metric_value, epoch+1)\n",
    "                        if metric_name == 'pred_seen_psnr':\n",
    "                            psnr_list.append(metric_value.max())\n",
    "                        print_str += f'{metric_name}: {RoundTensor(metric_value, 2)} | '\n",
    "                    best_metric_list[i] = best_metric_value\n",
    "                print(print_str, flush=True)\n",
    "                with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "                    f.write(print_str + '\\n')\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "        save_checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'state_dict': state_dict,\n",
    "            'optimizer': optimizer.state_dict(),   \n",
    "        }    \n",
    "        if local_rank in [0, None]:\n",
    "            torch.save(save_checkpoint, '{}/model_latest.pth'.format(args.outf))\n",
    "            if (epoch + 1) in args.finetune_ep:\n",
    "                args.cur_epoch = epoch + 1\n",
    "                args.train_time = str(datetime.now() - start)\n",
    "                \n",
    "                #Log time\n",
    "                raw_fu_avg_time_pepoch = raw_fu_encode_time / args.epochs\n",
    "                raw_fp_avg_time_pepoch = raw_fp_encode_time / args.epochs\n",
    "\n",
    "                args.train_encode_fu_time_pepoch = raw_fu_avg_time_pepoch\n",
    "                args.train_encode_fp_time_pepoch = raw_fp_avg_time_pepoch\n",
    "                args.train_encode_fu_time_total = raw_fu_encode_time\n",
    "                args.train_encode_fp_time_total = raw_fp_encode_time\n",
    "    \n",
    "                Dump2CSV(args, best_metric_list, results_list, psnr_list, f'epoch{epoch+1}_ratio{ratio}.csv')\n",
    "                torch.save(save_checkpoint, f'{args.outf}/epoch{epoch+1}_ratio{ratio}.pth')\n",
    "                if best_metric_list[0]==results_list[0]:\n",
    "                    torch.save(save_checkpoint, f'{args.outf}/model_best.pth')\n",
    "\n",
    "    if local_rank in [0, None]:\n",
    "        print(f\"Training complete in: {str(datetime.now() - start)}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Writing final results in CSV file\n",
    "def Dump2CSV(args, best_results_list, results_list, psnr_list, filename='results.csv'):\n",
    "    result_dict = {'Vid':args.vid, 'CurEpoch':args.cur_epoch, 'Train Time':args.train_time, \n",
    "        \"Compression Method\": args.compression_method, \"Compression Encoding Time\": args.compression_encoding_time,\n",
    "        \"Compression Decoding Time\": args.compression_decoding_time,\n",
    "        'FPS (Quantized, Full Pass)':args.fps_qt_fu, 'FPS (Quantized, Forward Pass)':args.fps_qt_fp,\n",
    "        'FPS (Full Model, Full Pass)':args.fps_raw_fu, 'FPS (Full Model, Forward Pass)':args.fps_raw_fp,\n",
    "        'Encoding / Training Time Total (Full Pass)':args.train_encode_fu_time_total, \n",
    "        'Encoding / Training Time Total (Forward Pass)':args.train_encode_fp_time_total,\n",
    "        'Encoding / Training Time per Epoch (Full Pass)':args.train_encode_fu_time_pepoch, \n",
    "        'Encoding / Training Time per Epoch (Forward Pass)':args.train_encode_fp_time_pepoch,\n",
    "        'FPS (Full Model, Full Pass)':args.fps_raw_fu, 'FPS (Full Model, Forward Pass)':args.fps_raw_fp,\n",
    "        'Split':args.data_split, 'Embed':args.embed, 'Crop': args.crop_list,\n",
    "        'Resize':args.resize_list, 'Lr_type':args.lr_type, 'LR (E-3)': args.lr*1e3, 'Batch':args.batchSize,\n",
    "        'Size (M)': f'{round(args.encoder_param, 2)}_{round(args.decoder_param, 2)}_{round(args.total_param, 2)}', \n",
    "        'ModelSize': args.modelsize, 'Epoch':args.epochs, 'Loss':args.loss, 'Act':args.act, 'Norm':args.norm,\n",
    "        'FC':args.fc_hw, 'Reduce':args.reduce, 'ENC_type':args.conv_type[0], 'ENC_strds':args.enc_strd_str, 'KS':args.ks,\n",
    "        'enc_dim':args.enc_dim, 'DEC':args.conv_type[1], 'DEC_strds':args.dec_strd_str, 'lower_width':args.lower_width,\n",
    "        'Quant':args.quant_str, 'bits/param':args.bits_per_param, 'bits/param w/ overhead':args.full_bits_per_param, \n",
    "        'bits/pixel':args.total_bpp, f'PSNR_list_{args.eval_freq}':','.join([RoundTensor(v, 2) for v in psnr_list]),\n",
    "        'Evaluation Quant Overall PSNR': args.eval_quant_overall_PSNR, 'Evaluation Orig Overall PSNR': args.eval_orig_overall_PSNR,\n",
    "        'Finetune Epoch': '_'.join([str(i) for i in args.finetune_ep]), 'Base Prune Ratio':args.base_prune_ratio, 'Prune Ratio Steo': args.prune_ratio_step,\n",
    "        'Estimated Prune Ratio':args.estimated_prune_ratio, 'Actual Prune Ratio':args.actual_prune_ratio,\n",
    "        'Pruned Zero Count':args.pruned_zero_count}\n",
    "    result_dict.update({f'best_{k}':RoundTensor(v, 4 if 'ssim' in k else 2) for k,v in zip(args.metric_names, best_results_list)})\n",
    "    result_dict.update({f'{k}':RoundTensor(v, 4 if 'ssim' in k else 2) for k,v in zip(args.metric_names, results_list) if 'pred' in k})\n",
    "    csv_path = os.path.join(args.outf, filename)\n",
    "    print(f'results dumped to {csv_path}')\n",
    "    pd.DataFrame(result_dict,index=[0]).to_csv(csv_path)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, full_dataloader, local_rank, args, \n",
    "    dump_vis=False, compression_method=None, epoch=0, ratio=0):\n",
    "    \n",
    "    #Init models (quantized and raw) and vars\n",
    "    img_embed_list = []\n",
    "    model_list, quant_ckt = quant_model(model, args)\n",
    "    metric_list = [[] for _ in range(len(args.metric_names))]\n",
    "    \n",
    "    \n",
    "    print(\"---\"*30)\n",
    "    print('[{}]'.format(\"Quant CKT\"))\n",
    "    quant_v_list = []\n",
    "    for k, layer_wt in quant_ckt.items():\n",
    "        quant_v_list.extend(layer_wt['quant'].flatten().tolist())\n",
    "        \n",
    "    #To array to work with\n",
    "    quant_v_list = np.array(quant_v_list)\n",
    "\n",
    "    plt.hist(quant_v_list,bins=100,range=[quant_v_list.min()-1,quant_v_list.max()+1],align='mid')\n",
    "    plt.show()\n",
    "\n",
    "    #Vars to track time\n",
    "    raw_fp_decode_time = 0\n",
    "    qt_fp_decode_time = 0\n",
    "    raw_fu_decode_time = 0\n",
    "    qt_fu_decode_time = 0\n",
    "    \n",
    "    for model_ind, cur_model in enumerate(model_list):\n",
    "        time_list = []\n",
    "        preds,gts = [],[]\n",
    "        cur_model.eval()\n",
    "        device = next(cur_model.parameters()).device\n",
    "        if dump_vis:\n",
    "            visual_dir = f'{args.outf}/ep{epoch}_ratio{ratio}_visualize_model' + ('_quant' if model_ind else '_orig')\n",
    "            print(f'Saving predictions to {visual_dir}...')\n",
    "            if not os.path.isdir(visual_dir):\n",
    "                os.makedirs(visual_dir)        \n",
    "            \n",
    "\n",
    "        #View\n",
    "        #Count zeros (most important)\n",
    "        raw_weights = []\n",
    "        \n",
    "        print('[{}]'.format('Quant' if model_ind else 'Orig'))\n",
    "        for k,v in cur_model.state_dict().items():\n",
    "\n",
    "            #Only count decoder and head layer\n",
    "            if 'encoder' not in k:\n",
    "                raw_weights.extend(v.ravel().cpu().numpy())\n",
    "            \n",
    "        #To array to work with\n",
    "        raw_weights = np.array(raw_weights)\n",
    "\n",
    "        plt.hist(raw_weights,bins=100,range=[raw_weights.min()-1,raw_weights.max()+1],align='mid')\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "        for i, sample in enumerate(full_dataloader):\n",
    "            \n",
    "            #Start track time to decode speed (full)\n",
    "            start_time_fu = time.time()\n",
    "            \n",
    "            img_data, img_down_data, norm_idx, img_idx = data_to_gpu(sample['img'], device), data_to_gpu(sample['img_down'], device),data_to_gpu(sample['norm_idx'], device), data_to_gpu(sample['idx'], device)\n",
    "                \n",
    "            #Overwrite for super-res\n",
    "            if args.super:\n",
    "                img_gt = img_data\n",
    "                img_data = img_down_data\n",
    "            else:\n",
    "                img_gt = img_data\n",
    "                \n",
    "            cur_input = norm_idx if 'pe' in args.embed else img_data\n",
    "            \n",
    "            #Start track time to decode speed (fp only)\n",
    "            start_time_fp = time.time()\n",
    "            \n",
    "            img_out, embed_list, dec_time, encoder_time = cur_model(cur_input, dequant_vid_embed[i] if model_ind else None)\n",
    "            \n",
    "            #End track time to decode speed (fp only)\n",
    "            end_time_fp = time.time()\n",
    "            \n",
    "            #End time (fu; this is inclusive of dataload and transform)\n",
    "            end_time_fu = time.time()\n",
    "            \n",
    "            #Compute time\n",
    "            time_delta_fp = end_time_fp - start_time_fp - encoder_time\n",
    "            time_delta_fu = end_time_fu - start_time_fu - encoder_time\n",
    "            \n",
    "            #Store\n",
    "            if model_ind: #quant model\n",
    "                qt_fu_decode_time += time_delta_fu\n",
    "                qt_fp_decode_time += time_delta_fp\n",
    "            else:\n",
    "                raw_fu_decode_time += time_delta_fu\n",
    "                raw_fp_decode_time += time_delta_fp               \n",
    "            \n",
    "\n",
    "            #This is FP time only\n",
    "            if model_ind == 0:\n",
    "                img_embed_list.append(embed_list[0])\n",
    "            \n",
    "            # collect decoding fps\n",
    "            time_list.append(dec_time)\n",
    "            if args.eval_fps:\n",
    "                time_list.pop()\n",
    "                for _ in range(100):\n",
    "                    img_out, embed_list, dec_time,encoder_time = cur_model(cur_input, embed_list[0])\n",
    "                    time_list.append(dec_time)\n",
    "\n",
    "            #Store pred frame and gt for overall PSNR computation (not just per batch or frame)\n",
    "            preds.append(img_out)\n",
    "            gts.append(img_gt)\n",
    "\n",
    "            # compute psnr and ms-ssim\n",
    "            pred_psnr, pred_ssim = psnr_fn_batch([img_out], img_gt), msssim_fn_batch([img_out], img_gt)\n",
    "            for metric_idx, cur_v in  enumerate([pred_psnr, pred_ssim]):\n",
    "                for batch_i, cur_img_idx in enumerate(img_idx):\n",
    "                    metric_idx_start = 2 if cur_img_idx in args.val_ind_list else 0\n",
    "                    metric_list[metric_idx_start+metric_idx+4*model_ind].append(cur_v[:,batch_i])\n",
    "\n",
    "            # dump predictions\n",
    "            if dump_vis:\n",
    "                for batch_ind, cur_img_idx in enumerate(img_idx):\n",
    "                    full_ind = i * args.batchSize + batch_ind\n",
    "                    dump_img_list = [img_data[batch_ind], img_out[batch_ind]]\n",
    "                    temp_psnr_list = ','.join([str(round(x[batch_ind].item(), 2)) for x in pred_psnr])\n",
    "                    if not args.super:\n",
    "                        dump_img_list = [img_data[batch_ind], img_out[batch_ind]]\n",
    "                    else: #super res, take gt instead of input as comparison,\n",
    "                        dump_img_list = [img_gt[batch_ind], img_out[batch_ind]]\n",
    "                    concat_img = torch.cat(dump_img_list, dim=2)    #img_out[batch_ind], \n",
    "                    save_image(concat_img, f'{visual_dir}/pred_{full_ind:04d}_{temp_psnr_list}.png')\n",
    "\n",
    "            # print eval results and add to log txt\n",
    "            \n",
    "\n",
    "            if i % args.print_freq == 0 or i == len(full_dataloader) - 1:\n",
    "                avg_time = sum(time_list) / len(time_list)\n",
    "                fps = args.batchSize / avg_time\n",
    "                print_str = '[{}] Rank:{}, Eval at Step [{}/{}] , FPS {}, '.format(\n",
    "                    datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"), local_rank, i+1, len(full_dataloader), round(fps, 1))\n",
    "                metric_name = ('quant' if model_ind else 'pred') + '_seen_psnr'\n",
    "                for v_name, v_list in zip(args.metric_names, metric_list):\n",
    "                    if metric_name in v_name:\n",
    "                        cur_value = torch.stack(v_list, dim=-1).mean(-1) if len(v_list) else torch.zeros(1)\n",
    "                        print_str += f'{v_name}: {RoundTensor(cur_value, 2)} | '\n",
    "                if local_rank in [0, None]:\n",
    "                    print(print_str, flush=True)\n",
    "                    with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "                        f.write(print_str + '\\n')\n",
    "        \n",
    "        # embedding quantization\n",
    "        if model_ind == 0:\n",
    "            vid_embed = torch.cat(img_embed_list, 0) \n",
    "            quant_embed, dequant_emved = quant_tensor(vid_embed, args.quant_embed_bit)\n",
    "            dequant_vid_embed = dequant_emved.split(args.batchSize, dim=0)\n",
    "            \n",
    "\n",
    "        # Collect results from \n",
    "        results_list = [torch.stack(v_list, dim=1).mean(1).cpu() if len(v_list) else torch.zeros(1) for v_list in metric_list]\n",
    "        h,w = img_data.shape[-2:]\n",
    "        cur_model.train()\n",
    "        if args.distributed and args.ngpus_per_node > 1:\n",
    "            for cur_v in results_list:\n",
    "                cur_v = all_reduce([cur_v.to(local_rank)])\n",
    "\n",
    "        # Dump predictions and concat into videos\n",
    "        if dump_vis and args.dump_videos:\n",
    "            gif_file = os.path.join(args.outf, 'gt_pred' + ('_quant.gif' if model_ind else '.gif'))\n",
    "            with imageio.get_writer(gif_file, mode='I') as writer:\n",
    "                for filename in sorted(os.listdir(visual_dir)):\n",
    "                    image = imageio.v2.imread(os.path.join(visual_dir, filename))\n",
    "                    writer.append_data(image)\n",
    "            if not args.dump_images:\n",
    "                shutil.rmtree(visual_dir)\n",
    "            # optimize(gif_file)\n",
    "\n",
    "        #Compute Overall PSNR\n",
    "#         overall_psnr = all_psnr(preds,gts)\n",
    "        overall_psnr = 0 #compute post-hoc; gets GPU OOM on large images if compute during training\n",
    "\n",
    "        if model_ind: #quantized model\n",
    "            args.eval_orig_overall_PSNR = overall_psnr #log\n",
    "        elif not model_ind: #original model\n",
    "            args.eval_quant_overall_PSNR = overall_psnr #log\n",
    "           \n",
    "    \n",
    "    #Store args            \n",
    "    qt_fu_avg_time = qt_fu_decode_time / len(full_dataloader)\n",
    "    qt_fp_avg_time = qt_fp_decode_time / len(full_dataloader)\n",
    "    raw_fu_avg_time = raw_fu_decode_time / len(full_dataloader)\n",
    "    raw_fp_avg_time = raw_fp_decode_time / len(full_dataloader)\n",
    "    \n",
    "    fps_qt_fu = 1 / qt_fu_avg_time\n",
    "    fps_qt_fp = 1 / qt_fp_avg_time\n",
    "    fps_raw_fu = 1 / raw_fu_avg_time\n",
    "    fps_raw_fp = 1 / raw_fp_avg_time\n",
    "    \n",
    "    args.fps_qt_fu = fps_qt_fu\n",
    "    args.fps_qt_fp = fps_qt_fp\n",
    "    args.fps_raw_fu = fps_raw_fu\n",
    "    args.fps_raw_fp = fps_raw_fp\n",
    "    \n",
    "        \n",
    "    # dump quantized checkpoint, and decoder\n",
    "    if local_rank in [0, None] and quant_ckt != None:\n",
    "        quant_vid = {'embed': quant_embed, 'model': quant_ckt}\n",
    "        norm_vid = {'embed': vid_embed, 'model':model}\n",
    "        \n",
    "        \n",
    "#         return quant_vid, norm_vid\n",
    "        \n",
    "        torch.save(quant_vid, f'{args.outf}/ep{epoch}_ratio{ratio}_quant_vid.pth')\n",
    "        torch.save(norm_vid, f'{args.outf}/ep{epoch}_ratio{ratio}_raw_vid.pth')\n",
    "        torch.jit.save(torch.jit.trace(HNeRVDecoder(model), (vid_embed[:2])), f'{args.outf}/ep{epoch}_ratio{ratio}_img_decoder.pth')\n",
    "        # huffman coding\n",
    "        if compression_method == 'huffman':\n",
    "            \n",
    "            quant_v_list = quant_embed['quant'].flatten().to(torch.uint8).tolist()\n",
    "            tmin_scale_len = quant_embed['min'].nelement() + quant_embed['scale'].nelement()\n",
    "            \n",
    "            \n",
    "            for k, layer_wt in quant_ckt.items():\n",
    "                quant_v_list.extend(layer_wt['quant'].flatten().to(torch.uint8).tolist())\n",
    "                tmin_scale_len += layer_wt['min'].nelement() + layer_wt['scale'].nelement()\n",
    "\n",
    "            # get the element name and its frequency\n",
    "            unique, counts = np.unique(quant_v_list, return_counts=True)\n",
    "            num_freq = dict(zip(unique, counts))\n",
    "            \n",
    "            #Start Track time (encode)\n",
    "            comp_encode_start_time = time.time()\n",
    "            \n",
    "            # generating HuffmanCoding table\n",
    "            codec = HuffmanCodec.from_data(quant_v_list)\n",
    "            sym_bit_dict = {}\n",
    "            for k, v in codec.get_code_table().items():\n",
    "                sym_bit_dict[k] = v[0]\n",
    "                \n",
    "            #End Track time (encode)\n",
    "            comp_encode_end_time = time.time()\n",
    "            delta_encode_comp = comp_encode_end_time - comp_encode_start_time\n",
    "            \n",
    "            encoded = codec.encode(quant_v_list) #keep separate\n",
    "\n",
    "            #Decompression\n",
    "            decomp_decode_start_time = time.time()\n",
    "            codec.decode(encoded)\n",
    "            decomp_decode_end_time = time.time()\n",
    "            delta_decode_decomp = decomp_decode_end_time - decomp_decode_start_time\n",
    "            \n",
    "            #Log\n",
    "            args.compression_encoding_time = delta_encode_comp \n",
    "            args.compression_decoding_time = delta_decode_decomp\n",
    "\n",
    "\n",
    "            # total bits for quantized embed + model weights\n",
    "            total_bits = 0\n",
    "            for num, freq in num_freq.items():\n",
    "                total_bits += freq * sym_bit_dict[num]\n",
    "            args.bits_per_param = total_bits / len(quant_v_list)\n",
    "            \n",
    "            print(\"sym_bit_dict\", sym_bit_dict, len(sym_bit_dict.keys()))\n",
    "            print(\"num_freq\", num_freq, len(sym_bit_dict.keys()))\n",
    "\n",
    "            \n",
    "            print(\">> [BEFORE SCALE] Total Bits: {} \".format(total_bits))\n",
    "            \n",
    "            # including the overhead for min and scale storage, \n",
    "            total_bits += tmin_scale_len * 16               #(16bits for float16)\n",
    "            args.full_bits_per_param = total_bits / len(quant_v_list)\n",
    "            \n",
    "            print(\">> [AFTER SCALE] Total Bits: {} \".format(total_bits))\n",
    "\n",
    "            # bits per pixel\n",
    "            args.total_bpp = total_bits / args.final_size / args.full_data_length / args.super_rate\n",
    "            print(f'>> After quantization and encoding: \\n bits per parameter: {round(args.full_bits_per_param, 2)}, bits per pixel: {round(args.total_bpp, 4)}')\n",
    "    # import pdb; pdb.set_trace; from IPython import embed; embed()     \n",
    "\n",
    "    return results_list, (h,w)\n",
    "\n",
    "\n",
    "def quant_model(model, args):\n",
    "    model_list = [deepcopy(model)]\n",
    "    if args.quant_model_bit == -1:\n",
    "        return model_list, None\n",
    "    else:\n",
    "        cur_model = deepcopy(model)\n",
    "        quant_ckt, cur_ckt = [cur_model.state_dict() for _ in range(2)]\n",
    "        encoder_k_list = []\n",
    "        for k,v in cur_ckt.items():\n",
    "            if 'encoder' in k:\n",
    "                encoder_k_list.append(k)\n",
    "            else:\n",
    "                quant_v, new_v = quant_tensor(v, args.quant_model_bit)\n",
    "                quant_ckt[k] = quant_v\n",
    "                cur_ckt[k] = new_v\n",
    "        for encoder_k in encoder_k_list:\n",
    "            del quant_ckt[encoder_k]\n",
    "        cur_model.load_state_dict(cur_ckt)\n",
    "        model_list.append(cur_model)\n",
    "        \n",
    "        return model_list, quant_ckt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playing with HNeRV config \n",
    "#Configs\n",
    "enc_strds = [5,4,4,3,2]\n",
    "dec_strds = [5,4,4,3,2]\n",
    "final_size = 960 * 540\n",
    "enc_dim = '64_16'\n",
    "full_data_length = 600\n",
    "modelsize = 1.5\n",
    "reduce = 1.2\n",
    "ks = '0_1_5'\n",
    "saturate_stages = -1\n",
    "lower_width = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code chunk 1\n",
    "total_enc_strds = np.prod(enc_strds)\n",
    "embed_hw = final_size / total_enc_strds**2\n",
    "enc_dim1, embed_ratio = [float(x) for x in enc_dim.split('_')]\n",
    "embed_dim = int(embed_ratio * modelsize * 1e6 / full_data_length / embed_hw) if embed_ratio < 1 else int(embed_ratio) \n",
    "embed_param = float(embed_dim) / total_enc_strds**2 * final_size * full_data_length\n",
    "enc_dim_ = f'{int(enc_dim1)}_{embed_dim}' \n",
    "fc_param = (np.prod(enc_strds) // np.prod(dec_strds))**2 * 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 2.25, 16, 21600.0, '64_16', 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_enc_strds, embed_hw, embed_dim, embed_param, enc_dim_, fc_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, should I have used full_size of 1920 x 960 instead :think:; will it work OOBox?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code chunk 2\n",
    "decoder_size = modelsize * 1e6 - embed_param\n",
    "ch_reduce = 1. / reduce\n",
    "dec_ks1, dec_ks2 = [int(x) for x in ks.split('_')[1:]]\n",
    "fix_ch_stages = len(dec_strds) if saturate_stages == -1 else saturate_stages\n",
    "a =  ch_reduce * sum([ch_reduce**(2*i) * s**2 * min((2*i + dec_ks1), dec_ks2)**2 for i,s in enumerate(dec_strds[:fix_ch_stages])])\n",
    "b =  embed_dim * fc_param \n",
    "c =  lower_width **2 * sum([s**2 * min(2*(fix_ch_stages + i) + dec_ks1, dec_ks2)  **2 for i, s in enumerate(dec_strds[fix_ch_stages:])])\n",
    "fc_dim = int(np.roots([a,b,c - decoder_size]).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1478400.0, 0.8333333333333334, 1, 5, 5, 347.09173604760457, 144, 0, 65)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_size, ch_reduce,dec_ks1, dec_ks2, fix_ch_stages, a, b, c, fc_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From HNeRV Class\n",
    "hnerv_hw = np.prod(enc_strds) // np.prod(dec_strds)\n",
    "hnerv_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try playing around with `enc_dim='64_16'` maybe change to `64_64` instead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv_torch_nerf",
   "language": "python",
   "name": "penv_torch_nerf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
