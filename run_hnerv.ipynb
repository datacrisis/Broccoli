{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import imageio\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model_all import VideoDataSet, HNeRV, HNeRVDecoder, TransformInput\n",
    "from hnerv_utils import *\n",
    "from torch.utils.data import Subset\n",
    "from copy import deepcopy\n",
    "from dahuffman import HuffmanCodec\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace argparse from default HNeRV code\n",
    "class Argparse():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 vid,\n",
    "                shuffle_data = True ,\n",
    "                data_split = '1_1_1' ,\n",
    "                crop_list = '640_1280' ,\n",
    "                resize_list = '-1',\n",
    "                embed =  '' ,\n",
    "                ks = '0_3_3' ,\n",
    "                enc_strds = [] ,\n",
    "                enc_dim = '64_16' ,\n",
    "                modelsize = 1.5 ,\n",
    "                saturate_stages = -1 ,\n",
    "                fc_hw = '9_16' ,\n",
    "                reduce = 1.2 ,\n",
    "                lower_width = 32 ,\n",
    "                dec_strds = [5, 3, 2, 2, 2] ,\n",
    "                num_blks = '1_1' ,\n",
    "                conv_type = ['convnext', 'pshuffel'],\n",
    "                norm = 'none' ,\n",
    "                act = 'gelu' ,\n",
    "                workers = 4, \n",
    "                batchSize = 1 ,\n",
    "                start_epoch = -1 ,\n",
    "                not_resume = True ,\n",
    "                epochs = 5 ,\n",
    "                block_params = '1_1' ,\n",
    "                lr = '0.001' ,\n",
    "                lr_type = 'cosine_0.1_1_0.1' ,\n",
    "                loss = 'Fusion6' ,\n",
    "                out_bias = 'tanh' ,\n",
    "                eval_only = False ,\n",
    "                eval_freq = '10' ,\n",
    "                quant_model_bit = 8 ,\n",
    "                quant_embed_bit = 6 ,\n",
    "                quant_axis = '0.0' ,\n",
    "                dump_images = False ,\n",
    "                dump_videos = False ,\n",
    "                eval_fps = False ,\n",
    "                encoder_file = '' ,\n",
    "                manualSeed = 1337 ,\n",
    "                distributed = False ,\n",
    "                debug = False ,\n",
    "                print_freq = 50 ,\n",
    "                weight = 'None' ,\n",
    "                overwrite = True ,\n",
    "                outf = 'unify' ,\n",
    "                suffix = ''):\n",
    "\n",
    "        #Set param\n",
    "        self.data_path = data_path\n",
    "        self.vid = vid\n",
    "        self.shuffle_data  = shuffle_data\n",
    "        self.data_split = data_split\n",
    "        self.crop_list = crop_list\n",
    "        self.resize_list = resize_list\n",
    "        self.embed = embed\n",
    "        self.ks = ks\n",
    "        self.enc_strds = enc_strds\n",
    "        self.enc_dim = enc_dim\n",
    "        self.modelsize = modelsize\n",
    "        self.saturate_stages = saturate_stages\n",
    "        self.fc_hw = fc_hw\n",
    "        self.reduce = reduce\n",
    "        self.lower_width = lower_width\n",
    "        self.dec_strds = dec_strds\n",
    "        self.num_blks = num_blks\n",
    "        self.conv_type = conv_type\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.workers = workers\n",
    "        self.batchSize = batchSize\n",
    "        self.start_epoch = start_epoch\n",
    "        self.not_resume = not_resume\n",
    "        self.epochs = epochs\n",
    "        self.block_params = block_params\n",
    "        self.lr = lr\n",
    "        self.lr_type = lr_type\n",
    "        self.loss = loss\n",
    "        self.out_bias = out_bias\n",
    "        self.eval_only = eval_only\n",
    "        self.eval_freq = eval_freq\n",
    "        self.quant_model_bit = quant_model_bit\n",
    "        self.quant_embed_bit = quant_embed_bit\n",
    "        self.quant_axis = quant_axis\n",
    "        self.dump_images = dump_images\n",
    "        self.dump_videos = dump_videos\n",
    "        self.eval_fps = eval_fps\n",
    "        self.encoder_file = encoder_file\n",
    "        self.manualSeed = manualSeed\n",
    "        self.distributed = distributed\n",
    "        self.debug = debug\n",
    "        self.print_freq = print_freq\n",
    "        self.weight = weight\n",
    "        self.overwrite = overwrite\n",
    "        self.outf = outf\n",
    "        self.suffix = suffix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init\n",
    "torch.set_printoptions(precision=4) \n",
    "\n",
    "args = Argparse(data_path='data/sony/',\n",
    "                vid='sony_new_test',\n",
    "                outf='sony_hnerv',\n",
    "                loss='L2',\n",
    "                enc_strds=np.array([5,4,4,2,2]),\n",
    "                dec_strds=np.array([5,4,4,2,2]),\n",
    "                enc_dim='64_16',\n",
    "                ks='0_1_5',\n",
    "                reduce=1.2,\n",
    "                epochs=10,\n",
    "                eval_freq=10,\n",
    "                lower_width=12,\n",
    "                batchSize=12,\n",
    "                lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = [i for i in text.split(\"add_argument\") if '--' in i]\n",
    "# key = [i.split(',')[0].split('--')[-1].strip(\"'')()-\") for i in key if '--' in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default = [i for i in text.split(\"add_argument\") if '--' in i]\n",
    "# default = [i.split('default') for i in default]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main func\n",
    "def main():\n",
    "    #Pre train check\n",
    "    if args.debug:\n",
    "        args.eval_freq = 1\n",
    "        args.outf = 'output/debug'\n",
    "    else:\n",
    "        args.outf = os.path.join('output', args.outf)\n",
    "\n",
    "    args.enc_strd_str, args.dec_strd_str = ','.join([str(x) for x in args.enc_strds]), ','.join([str(x) for x in args.dec_strds])\n",
    "    extra_str = 'Size{}_ENC_{}_{}_DEC_{}_{}_{}{}{}'.format(args.modelsize, args.conv_type[0], args.enc_strd_str, \n",
    "        args.conv_type[1], args.dec_strd_str, '' if args.norm == 'none' else f'_{args.norm}', \n",
    "        '_dist' if args.distributed else '', '_shuffle_data' if args.shuffle_data else '',)\n",
    "    args.quant_str = f'quant_M{args.quant_model_bit}_E{args.quant_embed_bit}'\n",
    "    embed_str = f'{args.embed}_Dim{args.enc_dim}'\n",
    "    exp_id = f'{args.vid}/{args.data_split}_{embed_str}_FC{args.fc_hw}_KS{args.ks}_RED{args.reduce}_low{args.lower_width}_blk{args.num_blks}' + \\\n",
    "            f'_e{args.epochs}_b{args.batchSize}_{args.quant_str}_lr{args.lr}_{args.lr_type}_{args.loss}_{extra_str}{args.act}{args.block_params}{args.suffix}'\n",
    "    args.exp_id = exp_id\n",
    "\n",
    "    args.outf = os.path.join(args.outf, exp_id)\n",
    "    if args.overwrite and os.path.isdir(args.outf):\n",
    "        print('Will overwrite the existing output dir!')\n",
    "        shutil.rmtree(args.outf)\n",
    "\n",
    "    if not os.path.isdir(args.outf):\n",
    "        os.makedirs(args.outf)\n",
    "\n",
    "    port = hash(args.exp_id) % 20000 + 10000\n",
    "    args.init_method =  f'tcp://127.0.0.1:{port}'\n",
    "    print(f'init_method: {args.init_method}', flush=True)\n",
    "\n",
    "    torch.set_printoptions(precision=2) \n",
    "    args.ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "    if args.distributed and args.ngpus_per_node > 1:\n",
    "        mp.spawn(train, nprocs=args.ngpus_per_node, args=(args,))\n",
    "    else:\n",
    "        train(None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper func\n",
    "def data_to_gpu(x, device):\n",
    "    return x.to(device)\n",
    "\n",
    "def train(local_rank, args):\n",
    "    cudnn.benchmark = True\n",
    "    torch.manual_seed(args.manualSeed)\n",
    "    np.random.seed(args.manualSeed)\n",
    "    random.seed(args.manualSeed)\n",
    "\n",
    "    if args.distributed and args.ngpus_per_node > 1:\n",
    "        torch.distributed.init_process_group(\n",
    "            backend='nccl',\n",
    "            init_method=args.init_method,\n",
    "            world_size=args.ngpus_per_node,\n",
    "            rank=local_rank,\n",
    "        )\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        assert torch.distributed.is_initialized()        \n",
    "        args.batchSize = int(args.batchSize / args.ngpus_per_node)\n",
    "\n",
    "    args.metric_names = ['pred_seen_psnr', 'pred_seen_ssim', 'pred_unseen_psnr', 'pred_unseen_ssim',\n",
    "        'quant_seen_psnr', 'quant_seen_ssim', 'quant_unseen_psnr', 'quant_unseen_ssim']\n",
    "    best_metric_list = [torch.tensor(0) for _ in range(len(args.metric_names))]\n",
    "\n",
    "    # setup dataloader    \n",
    "    full_dataset = VideoDataSet(args)\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(full_dataset) if args.distributed else None\n",
    "    full_dataloader = torch.utils.data.DataLoader(full_dataset, batch_size=args.batchSize, shuffle=(sampler is None),\n",
    "            num_workers=args.workers, pin_memory=True, sampler=sampler, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "    args.final_size = full_dataset.final_size\n",
    "    args.full_data_length = len(full_dataset)\n",
    "    split_num_list = [int(x) for x in args.data_split.split('_')]\n",
    "    train_ind_list, args.val_ind_list = data_split(list(range(args.full_data_length)), split_num_list, args.shuffle_data, 0)\n",
    "    args.dump_vis = (args.dump_images or args.dump_videos)\n",
    "\n",
    "    #  Make sure the testing dataset is fixed for every run\n",
    "    train_dataset =  Subset(full_dataset, train_ind_list)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batchSize, shuffle=(train_sampler is None),\n",
    "         num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # Compute the parameter number\n",
    "    if 'pe' in args.embed or 'le' in args.embed:\n",
    "        embed_param = 0\n",
    "        embed_dim = int(args.embed.split('_')[-1]) * 2\n",
    "        fc_param = np.prod([int(x) for x in args.fc_hw.split('_')])\n",
    "    else:\n",
    "        total_enc_strds = np.prod(args.enc_strds)\n",
    "        embed_hw = args.final_size / total_enc_strds**2\n",
    "        enc_dim1, embed_ratio = [float(x) for x in args.enc_dim.split('_')]\n",
    "        embed_dim = int(embed_ratio * args.modelsize * 1e6 / args.full_data_length / embed_hw) if embed_ratio < 1 else int(embed_ratio) \n",
    "        embed_param = float(embed_dim) / total_enc_strds**2 * args.final_size * args.full_data_length\n",
    "        args.enc_dim = f'{int(enc_dim1)}_{embed_dim}' \n",
    "        fc_param = (np.prod(args.enc_strds) // np.prod(args.dec_strds))**2 * 9\n",
    "\n",
    "    decoder_size = args.modelsize * 1e6 - embed_param\n",
    "    ch_reduce = 1. / args.reduce\n",
    "    dec_ks1, dec_ks2 = [int(x) for x in args.ks.split('_')[1:]]\n",
    "    fix_ch_stages = len(args.dec_strds) if args.saturate_stages == -1 else args.saturate_stages\n",
    "    a =  ch_reduce * sum([ch_reduce**(2*i) * s**2 * min((2*i + dec_ks1), dec_ks2)**2 for i,s in enumerate(args.dec_strds[:fix_ch_stages])])\n",
    "    b =  embed_dim * fc_param \n",
    "    c =  args.lower_width **2 * sum([s**2 * min(2*(fix_ch_stages + i) + dec_ks1, dec_ks2)  **2 for i, s in enumerate(args.dec_strds[fix_ch_stages:])])\n",
    "    args.fc_dim = int(np.roots([a,b,c - decoder_size]).max())\n",
    "\n",
    "    # Building model\n",
    "    model = HNeRV(args)\n",
    "\n",
    "    ##### get model params and flops #####\n",
    "    if local_rank in [0, None]:\n",
    "        encoder_param = (sum([p.data.nelement() for p in model.encoder.parameters()]) / 1e6) \n",
    "        decoder_param = (sum([p.data.nelement() for p in model.decoder.parameters()]) / 1e6) \n",
    "        total_param = decoder_param + embed_param / 1e6\n",
    "        args.encoder_param, args.decoder_param, args.total_param = encoder_param, decoder_param, total_param\n",
    "        param_str = f'Encoder_{round(encoder_param, 2)}M_Decoder_{round(decoder_param, 2)}M_Total_{round(total_param, 2)}M'\n",
    "        print(f'{args}\\n {model}\\n {param_str}', flush=True)\n",
    "        with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "            f.write(str(model) + '\\n' + f'{param_str}\\n')\n",
    "        writer = SummaryWriter(os.path.join(args.outf, param_str, 'tensorboard'))\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    # distrite model to gpu or parallel\n",
    "    print(\"Use GPU: {} for training\".format(local_rank))\n",
    "    if args.distributed and args.ngpus_per_node > 1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model.to(local_rank), device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False)\n",
    "    elif args.ngpus_per_node > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    elif torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=0.)\n",
    "    args.transform_func = TransformInput(args)\n",
    "\n",
    "    # resume from args.weight\n",
    "    checkpoint = None\n",
    "    loc = 'cuda:{}'.format(local_rank if local_rank is not None else 0)\n",
    "    if args.weight != 'None':\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.weight))\n",
    "        checkpoint_path = args.weight\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        orig_ckt = checkpoint['state_dict']\n",
    "        new_ckt={k.replace('blocks.0.',''):v for k,v in orig_ckt.items()} \n",
    "        if 'module' in list(orig_ckt.keys())[0] and not hasattr(model, 'module'):\n",
    "            new_ckt={k.replace('module.',''):v for k,v in new_ckt.items()}\n",
    "            model.load_state_dict(new_ckt, strict=False)\n",
    "        elif 'module' not in list(orig_ckt.keys())[0] and hasattr(model, 'module'):\n",
    "            model.module.load_state_dict(new_ckt, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(new_ckt, strict=False)\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.weight, checkpoint['epoch']))        \n",
    "\n",
    "    # resume from model_latest\n",
    "    if not args.not_resume:\n",
    "        checkpoint_path = os.path.join(args.outf, 'model_latest.pth')\n",
    "        if os.path.isfile(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> Auto resume loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> No resume checkpoint found at '{}'\".format(checkpoint_path))\n",
    "\n",
    "    if args.start_epoch < 0:\n",
    "        if checkpoint is not None:\n",
    "            args.start_epoch = checkpoint['epoch'] \n",
    "        args.start_epoch = max(args.start_epoch, 0)\n",
    "\n",
    "    if args.eval_only:\n",
    "        print_str = 'Evaluation ... \\n {} Results for checkpoint: {}\\n'.format(datetime.now().strftime('%Y_%m_%d_%H_%M_%S'), args.weight)\n",
    "        results_list, hw = evaluate(model, full_dataloader, local_rank, args, args.dump_vis, huffman_coding=True)\n",
    "        print_str = f'PSNR for output {hw} for quant {args.quant_str}: '\n",
    "        for i, (metric_name, best_metric_value, metric_value) in enumerate(zip(args.metric_names, best_metric_list, results_list)):\n",
    "            best_metric_value = best_metric_value if best_metric_value > metric_value.max() else metric_value.max()\n",
    "            cur_v = RoundTensor(best_metric_value, 2 if 'psnr' in metric_name else 4)\n",
    "            print_str += f'best_{metric_name}: {cur_v} | '\n",
    "            best_metric_list[i] = best_metric_value\n",
    "        if local_rank in [0, None]:\n",
    "            print(print_str, flush=True)\n",
    "            with open('{}/eval.txt'.format(args.outf), 'a') as f:\n",
    "                f.write(print_str + '\\n\\n')        \n",
    "            args.train_time, args.cur_epoch = 0, args.epochs\n",
    "            Dump2CSV(args, best_metric_list, results_list, [torch.tensor(0)], 'eval.csv')\n",
    "\n",
    "        return\n",
    "\n",
    "    # Training\n",
    "    start = datetime.now()\n",
    "\n",
    "    psnr_list = []\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        model.train()       \n",
    "        epoch_start_time = datetime.now()\n",
    "        pred_psnr_list = []\n",
    "        # iterate over dataloader\n",
    "        device = next(model.parameters()).device\n",
    "        for i, sample in enumerate(train_dataloader):\n",
    "            img_data, norm_idx, img_idx = data_to_gpu(sample['img'], device), data_to_gpu(sample['norm_idx'], device), data_to_gpu(sample['idx'], device)\n",
    "            if i > 10 and args.debug:\n",
    "                break\n",
    "\n",
    "            # forward and backward\n",
    "            img_data, img_gt, inpaint_mask = args.transform_func(img_data)\n",
    "            cur_input = norm_idx if 'pe' in args.embed else img_data\n",
    "            cur_epoch = (epoch + float(i) / len(train_dataloader)) / args.epochs\n",
    "            lr = adjust_lr(optimizer, cur_epoch, args)\n",
    "            img_out, _, _ = model(cur_input)\n",
    "            final_loss = loss_fn(img_out*inpaint_mask, img_gt*inpaint_mask, args.loss)      \n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_psnr_list.append(psnr_fn_single(img_out.detach(), img_gt)) \n",
    "            if i % args.print_freq == 0 or i == len(train_dataloader) - 1:\n",
    "                pred_psnr = torch.cat(pred_psnr_list).mean()\n",
    "                print_str = '[{}] Rank:{}, Epoch[{}/{}], Step [{}/{}], lr:{:.2e} pred_PSNR: {}'.format(\n",
    "                    datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"), local_rank, epoch+1, args.epochs, i+1, len(train_dataloader), lr, \n",
    "                    RoundTensor(pred_psnr, 2))\n",
    "                print(print_str, flush=True)\n",
    "                if local_rank in [0, None]:\n",
    "                    with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "                        f.write(print_str + '\\n')\n",
    "\n",
    "        # collect numbers from other gpus\n",
    "        if args.distributed and args.ngpus_per_node > 1:\n",
    "            pred_psnr = all_reduce([pred_psnr.to(local_rank)])\n",
    "\n",
    "        # ADD train_PSNR TO TENSORBOARD\n",
    "        if local_rank in [0, None]:\n",
    "            h, w = img_out.shape[-2:]\n",
    "            writer.add_scalar(f'Train/pred_PSNR_{h}X{w}', pred_psnr, epoch+1)\n",
    "            writer.add_scalar('Train/lr', lr, epoch+1)\n",
    "            epoch_end_time = datetime.now()\n",
    "            print(\"Time/epoch: \\tCurrent:{:.2f} \\tAverage:{:.2f}\".format( (epoch_end_time - epoch_start_time).total_seconds(), \\\n",
    "                    (epoch_end_time - start).total_seconds() / (epoch + 1 - args.start_epoch) ))\n",
    "\n",
    "        # evaluation\n",
    "        if (epoch + 1) % args.eval_freq == 0 or (args.epochs - epoch) in [1, 3, 5]:\n",
    "            results_list, hw = evaluate(model, full_dataloader, local_rank, args, \n",
    "                args.dump_vis if epoch == args.epochs - 1 else False, \n",
    "                True if epoch == args.epochs - 1 else False)            \n",
    "            if local_rank in [0, None]:\n",
    "                # ADD val_PSNR TO TENSORBOARD\n",
    "                print_str = f'Eval at epoch {epoch+1} for {hw}: '\n",
    "                for i, (metric_name, best_metric_value, metric_value) in enumerate(zip(args.metric_names, best_metric_list, results_list)):\n",
    "                    best_metric_value = best_metric_value if best_metric_value > metric_value.max() else metric_value.max()\n",
    "                    if 'psnr' in metric_name:\n",
    "                        writer.add_scalar(f'Val/{metric_name}_{hw}', metric_value.max(), epoch+1)\n",
    "                        writer.add_scalar(f'Val/best_{metric_name}_{hw}', best_metric_value, epoch+1)\n",
    "                        if metric_name == 'pred_seen_psnr':\n",
    "                            psnr_list.append(metric_value.max())\n",
    "                        print_str += f'{metric_name}: {RoundTensor(metric_value, 2)} | '\n",
    "                    best_metric_list[i] = best_metric_value\n",
    "                print(print_str, flush=True)\n",
    "                with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "                    f.write(print_str + '\\n')\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "        save_checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'state_dict': state_dict,\n",
    "            'optimizer': optimizer.state_dict(),   \n",
    "        }    \n",
    "        if local_rank in [0, None]:\n",
    "            torch.save(save_checkpoint, '{}/model_latest.pth'.format(args.outf))\n",
    "            if (epoch + 1) % args.epochs == 0:\n",
    "                args.cur_epoch = epoch + 1\n",
    "                args.train_time = str(datetime.now() - start)\n",
    "                Dump2CSV(args, best_metric_list, results_list, psnr_list, f'epoch{epoch+1}.csv')\n",
    "                torch.save(save_checkpoint, f'{args.outf}/epoch{epoch+1}.pth')\n",
    "                if best_metric_list[0]==results_list[0]:\n",
    "                    torch.save(save_checkpoint, f'{args.outf}/model_best.pth')\n",
    "\n",
    "    if local_rank in [0, None]:\n",
    "        print(f\"Training complete in: {str(datetime.now() - start)}\")\n",
    "\n",
    "\n",
    "# Writing final results in CSV file\n",
    "def Dump2CSV(args, best_results_list, results_list, psnr_list, filename='results.csv'):\n",
    "    result_dict = {'Vid':args.vid, 'CurEpoch':args.cur_epoch, 'Time':args.train_time, \n",
    "        'FPS':args.fps, 'Split':args.data_split, 'Embed':args.embed, 'Crop': args.crop_list,\n",
    "        'Resize':args.resize_list, 'Lr_type':args.lr_type, 'LR (E-3)': args.lr*1e3, 'Batch':args.batchSize,\n",
    "        'Size (M)': f'{round(args.encoder_param, 2)}_{round(args.decoder_param, 2)}_{round(args.total_param, 2)}', \n",
    "        'ModelSize': args.modelsize, 'Epoch':args.epochs, 'Loss':args.loss, 'Act':args.act, 'Norm':args.norm,\n",
    "        'FC':args.fc_hw, 'Reduce':args.reduce, 'ENC_type':args.conv_type[0], 'ENC_strds':args.enc_strd_str, 'KS':args.ks,\n",
    "        'enc_dim':args.enc_dim, 'DEC':args.conv_type[1], 'DEC_strds':args.dec_strd_str, 'lower_width':args.lower_width,\n",
    "         'Quant':args.quant_str, 'bits/param':args.bits_per_param, 'bits/param w/ overhead':args.full_bits_per_param, \n",
    "        'bits/pixel':args.total_bpp, f'PSNR_list_{args.eval_freq}':','.join([RoundTensor(v, 2) for v in psnr_list]),}\n",
    "    result_dict.update({f'best_{k}':RoundTensor(v, 4 if 'ssim' in k else 2) for k,v in zip(args.metric_names, best_results_list)})\n",
    "    result_dict.update({f'{k}':RoundTensor(v, 4 if 'ssim' in k else 2) for k,v in zip(args.metric_names, results_list) if 'pred' in k})\n",
    "    csv_path = os.path.join(args.outf, filename)\n",
    "    print(f'results dumped to {csv_path}')\n",
    "    pd.DataFrame(result_dict,index=[0]).to_csv(csv_path)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, full_dataloader, local_rank, args, \n",
    "    dump_vis=False, huffman_coding=False):\n",
    "    img_embed_list = []\n",
    "    model_list, quant_ckt = quant_model(model, args)\n",
    "    metric_list = [[] for _ in range(len(args.metric_names))]\n",
    "    for model_ind, cur_model in enumerate(model_list):\n",
    "        time_list = []\n",
    "        cur_model.eval()\n",
    "        device = next(cur_model.parameters()).device\n",
    "        if dump_vis:\n",
    "            visual_dir = f'{args.outf}/visualize_model' + ('_quant' if model_ind else '_orig')\n",
    "            print(f'Saving predictions to {visual_dir}...')\n",
    "            if not os.path.isdir(visual_dir):\n",
    "                os.makedirs(visual_dir)        \n",
    "\n",
    "        for i, sample in enumerate(full_dataloader):\n",
    "            img_data, norm_idx, img_idx = data_to_gpu(sample['img'], device), data_to_gpu(sample['norm_idx'], device), data_to_gpu(sample['idx'], device)\n",
    "            if i > 10 and args.debug:\n",
    "                break\n",
    "            img_data, img_gt, inpaint_mask = args.transform_func(img_data)\n",
    "            cur_input = norm_idx if 'pe' in args.embed else img_data\n",
    "            img_out, embed_list, dec_time = cur_model(cur_input, dequant_vid_embed[i] if model_ind else None)\n",
    "            if model_ind == 0:\n",
    "                img_embed_list.append(embed_list[0])\n",
    "            \n",
    "            # collect decoding fps\n",
    "            time_list.append(dec_time)\n",
    "            if args.eval_fps:\n",
    "                time_list.pop()\n",
    "                for _ in range(100):\n",
    "                    img_out, embed_list, dec_time = cur_model(cur_input, embed_list[0])\n",
    "                    time_list.append(dec_time)\n",
    "\n",
    "            # compute psnr and ms-ssim\n",
    "            pred_psnr, pred_ssim = psnr_fn_batch([img_out], img_gt), msssim_fn_batch([img_out], img_gt)\n",
    "            for metric_idx, cur_v in  enumerate([pred_psnr, pred_ssim]):\n",
    "                for batch_i, cur_img_idx in enumerate(img_idx):\n",
    "                    metric_idx_start = 2 if cur_img_idx in args.val_ind_list else 0\n",
    "                    metric_list[metric_idx_start+metric_idx+4*model_ind].append(cur_v[:,batch_i])\n",
    "\n",
    "            # dump predictions\n",
    "            if dump_vis:\n",
    "                for batch_ind, cur_img_idx in enumerate(img_idx):\n",
    "                    full_ind = i * args.batchSize + batch_ind\n",
    "                    dump_img_list = [img_data[batch_ind], img_out[batch_ind]]\n",
    "                    temp_psnr_list = ','.join([str(round(x[batch_ind].item(), 2)) for x in pred_psnr])\n",
    "                    concat_img = torch.cat(dump_img_list, dim=2)    #img_out[batch_ind], \n",
    "                    save_image(concat_img, f'{visual_dir}/pred_{full_ind:04d}_{temp_psnr_list}.png')\n",
    "\n",
    "            # print eval results and add to log txt\n",
    "            if i % args.print_freq == 0 or i == len(full_dataloader) - 1:\n",
    "                avg_time = sum(time_list) / len(time_list)\n",
    "                fps = args.batchSize / avg_time\n",
    "                print_str = '[{}] Rank:{}, Eval at Step [{}/{}] , FPS {}, '.format(\n",
    "                    datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"), local_rank, i+1, len(full_dataloader), round(fps, 1))\n",
    "                metric_name = ('quant' if model_ind else 'pred') + '_seen_psnr'\n",
    "                for v_name, v_list in zip(args.metric_names, metric_list):\n",
    "                    if metric_name in v_name:\n",
    "                        cur_value = torch.stack(v_list, dim=-1).mean(-1) if len(v_list) else torch.zeros(1)\n",
    "                        print_str += f'{v_name}: {RoundTensor(cur_value, 2)} | '\n",
    "                if local_rank in [0, None]:\n",
    "                    print(print_str, flush=True)\n",
    "                    with open('{}/rank0.txt'.format(args.outf), 'a') as f:\n",
    "                        f.write(print_str + '\\n')\n",
    "        \n",
    "        # embedding quantization\n",
    "        if model_ind == 0:\n",
    "            vid_embed = torch.cat(img_embed_list, 0) \n",
    "            quant_embed, dequant_emved = quant_tensor(vid_embed, args.quant_embed_bit)\n",
    "            dequant_vid_embed = dequant_emved.split(args.batchSize, dim=0)\n",
    "\n",
    "        # Collect results from \n",
    "        results_list = [torch.stack(v_list, dim=1).mean(1).cpu() if len(v_list) else torch.zeros(1) for v_list in metric_list]\n",
    "        args.fps = fps\n",
    "        h,w = img_data.shape[-2:]\n",
    "        cur_model.train()\n",
    "        if args.distributed and args.ngpus_per_node > 1:\n",
    "            for cur_v in results_list:\n",
    "                cur_v = all_reduce([cur_v.to(local_rank)])\n",
    "\n",
    "        # Dump predictions and concat into videos\n",
    "        if dump_vis and args.dump_videos:\n",
    "            gif_file = os.path.join(args.outf, 'gt_pred' + ('_quant.gif' if model_ind else '.gif'))\n",
    "            with imageio.get_writer(gif_file, mode='I') as writer:\n",
    "                for filename in sorted(os.listdir(visual_dir)):\n",
    "                    image = imageio.v2.imread(os.path.join(visual_dir, filename))\n",
    "                    writer.append_data(image)\n",
    "            if not args.dump_images:\n",
    "                shutil.rmtree(visual_dir)\n",
    "            # optimize(gif_file)\n",
    "        \n",
    "    # dump quantized checkpoint, and decoder\n",
    "    if local_rank in [0, None] and quant_ckt != None:\n",
    "        quant_vid = {'embed': quant_embed, 'model': quant_ckt}\n",
    "        torch.save(quant_vid, f'{args.outf}/quant_vid.pth')\n",
    "        torch.jit.save(torch.jit.trace(HNeRVDecoder(model), (vid_embed[:2])), f'{args.outf}/img_decoder.pth')\n",
    "        # huffman coding\n",
    "        if huffman_coding:\n",
    "            quant_v_list = quant_embed['quant'].flatten().tolist()\n",
    "            tmin_scale_len = quant_embed['min'].nelement() + quant_embed['scale'].nelement()\n",
    "            for k, layer_wt in quant_ckt.items():\n",
    "                quant_v_list.extend(layer_wt['quant'].flatten().tolist())\n",
    "                tmin_scale_len += layer_wt['min'].nelement() + layer_wt['scale'].nelement()\n",
    "\n",
    "            # get the element name and its frequency\n",
    "            unique, counts = np.unique(quant_v_list, return_counts=True)\n",
    "            num_freq = dict(zip(unique, counts))\n",
    "\n",
    "            # generating HuffmanCoding table\n",
    "            codec = HuffmanCodec.from_data(quant_v_list)\n",
    "            sym_bit_dict = {}\n",
    "            for k, v in codec.get_code_table().items():\n",
    "                sym_bit_dict[k] = v[0]\n",
    "\n",
    "            # total bits for quantized embed + model weights\n",
    "            total_bits = 0\n",
    "            for num, freq in num_freq.items():\n",
    "                total_bits += freq * sym_bit_dict[num]\n",
    "            args.bits_per_param = total_bits / len(quant_v_list)\n",
    "            \n",
    "            # including the overhead for min and scale storage, \n",
    "            total_bits += tmin_scale_len * 16               #(16bits for float16)\n",
    "            args.full_bits_per_param = total_bits / len(quant_v_list)\n",
    "\n",
    "            # bits per pixel\n",
    "            args.total_bpp = total_bits / args.final_size / args.full_data_length\n",
    "            print(f'After quantization and encoding: \\n bits per parameter: {round(args.full_bits_per_param, 2)}, bits per pixel: {round(args.total_bpp, 4)}')\n",
    "    # import pdb; pdb.set_trace; from IPython import embed; embed()     \n",
    "\n",
    "    return results_list, (h,w)\n",
    "\n",
    "\n",
    "def quant_model(model, args):\n",
    "    model_list = [deepcopy(model)]\n",
    "    if args.quant_model_bit == -1:\n",
    "        return model_list, None\n",
    "    else:\n",
    "        cur_model = deepcopy(model)\n",
    "        quant_ckt, cur_ckt = [cur_model.state_dict() for _ in range(2)]\n",
    "        encoder_k_list = []\n",
    "        for k,v in cur_ckt.items():\n",
    "            if 'encoder' in k:\n",
    "                encoder_k_list.append(k)\n",
    "            else:\n",
    "                quant_v, new_v = quant_tensor(v, args.quant_model_bit)\n",
    "                quant_ckt[k] = quant_v\n",
    "                cur_ckt[k] = new_v\n",
    "        for encoder_k in encoder_k_list:\n",
    "            del quant_ckt[encoder_k]\n",
    "        cur_model.load_state_dict(cur_ckt)\n",
    "        model_list.append(cur_model)\n",
    "        \n",
    "        return model_list, quant_ckt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will overwrite the existing output dir!\n",
      "init_method: tcp://127.0.0.1:26862\n",
      "<__main__.Argparse object at 0x1459f30ae730>\n",
      " HNeRV(\n",
      "  (encoder): ConvNeXt(\n",
      "    (downsample_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(5, 5))\n",
      "        (1): LayerNorm()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): LayerNorm()\n",
      "        (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): LayerNorm()\n",
      "        (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): LayerNorm()\n",
      "        (1): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): LayerNorm()\n",
      "        (1): Conv2d(64, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (stages): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Block(\n",
      "          (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "          (norm): LayerNorm()\n",
      "          (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Block(\n",
      "          (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "          (norm): LayerNorm()\n",
      "          (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Block(\n",
      "          (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "          (norm): LayerNorm()\n",
      "          (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Block(\n",
      "          (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "          (norm): LayerNorm()\n",
      "          (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Block(\n",
      "          (dwconv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=16)\n",
      "          (norm): LayerNorm()\n",
      "          (pwconv1): Linear(in_features=16, out_features=64, bias=True)\n",
      "          (act): GELU()\n",
      "          (pwconv2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): NeRVBlock(\n",
      "      (conv): DownConv(\n",
      "        (downconv): Conv2d(16, 68, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm): Identity()\n",
      "      (act): GELU()\n",
      "    )\n",
      "    (1): NeRVBlock(\n",
      "      (conv): UpConv(\n",
      "        (upconv): Sequential(\n",
      "          (0): Conv2d(68, 1425, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PixelShuffle(upscale_factor=5)\n",
      "        )\n",
      "      )\n",
      "      (norm): Identity()\n",
      "      (act): GELU()\n",
      "    )\n",
      "    (2): NeRVBlock(\n",
      "      (conv): UpConv(\n",
      "        (upconv): Sequential(\n",
      "          (0): Conv2d(57, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): PixelShuffle(upscale_factor=4)\n",
      "        )\n",
      "      )\n",
      "      (norm): Identity()\n",
      "      (act): GELU()\n",
      "    )\n",
      "    (3): NeRVBlock(\n",
      "      (conv): UpConv(\n",
      "        (upconv): Sequential(\n",
      "          (0): Conv2d(48, 640, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): PixelShuffle(upscale_factor=4)\n",
      "        )\n",
      "      )\n",
      "      (norm): Identity()\n",
      "      (act): GELU()\n",
      "    )\n",
      "    (4): NeRVBlock(\n",
      "      (conv): UpConv(\n",
      "        (upconv): Sequential(\n",
      "          (0): Conv2d(40, 132, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): PixelShuffle(upscale_factor=2)\n",
      "        )\n",
      "      )\n",
      "      (norm): Identity()\n",
      "      (act): GELU()\n",
      "    )\n",
      "    (5): NeRVBlock(\n",
      "      (conv): UpConv(\n",
      "        (upconv): Sequential(\n",
      "          (0): Conv2d(33, 112, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): PixelShuffle(upscale_factor=2)\n",
      "        )\n",
      "      )\n",
      "      (norm): Identity()\n",
      "      (act): GELU()\n",
      "    )\n",
      "  )\n",
      "  (head_layer): Conv2d(28, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      " Encoder_0.31M_Decoder_1.49M_Total_1.51M\n",
      "Use GPU: None for training\n",
      "[2023/03/25 22:10:11] Rank:None, Epoch[1/10], Step [1/12], lr:1.00e-04 pred_PSNR: 8.45\n",
      "[2023/03/25 22:10:15] Rank:None, Epoch[1/10], Step [12/12], lr:9.25e-04 pred_PSNR: 9.47\n",
      "Time/epoch: \tCurrent:5.40 \tAverage:5.40\n",
      "[2023/03/25 22:10:17] Rank:None, Epoch[2/10], Step [1/12], lr:1.00e-03 pred_PSNR: 12.38\n",
      "[2023/03/25 22:10:21] Rank:None, Epoch[2/10], Step [12/12], lr:9.75e-04 pred_PSNR: 11.88\n",
      "Time/epoch: \tCurrent:5.40 \tAverage:5.42\n",
      "[2023/03/25 22:10:22] Rank:None, Epoch[3/10], Step [1/12], lr:9.70e-04 pred_PSNR: 12.97\n",
      "[2023/03/25 22:10:26] Rank:None, Epoch[3/10], Step [12/12], lr:8.92e-04 pred_PSNR: 13.51\n",
      "Time/epoch: \tCurrent:5.35 \tAverage:5.42\n",
      "[2023/03/25 22:10:27] Rank:None, Epoch[4/10], Step [1/12], lr:8.83e-04 pred_PSNR: 13.83\n",
      "[2023/03/25 22:10:32] Rank:None, Epoch[4/10], Step [12/12], lr:7.62e-04 pred_PSNR: 14.34\n",
      "Time/epoch: \tCurrent:5.34 \tAverage:5.41\n",
      "[2023/03/25 22:10:33] Rank:None, Epoch[5/10], Step [1/12], lr:7.50e-04 pred_PSNR: 13.55\n",
      "[2023/03/25 22:10:37] Rank:None, Epoch[5/10], Step [12/12], lr:6.01e-04 pred_PSNR: 14.81\n",
      "Time/epoch: \tCurrent:5.35 \tAverage:5.40\n",
      "[2023/03/25 22:10:38] Rank:None, Epoch[6/10], Step [1/12], lr:5.87e-04 pred_PSNR: 15.15\n",
      "[2023/03/25 22:10:42] Rank:None, Epoch[6/10], Step [12/12], lr:4.28e-04 pred_PSNR: 15.47\n",
      "Time/epoch: \tCurrent:5.44 \tAverage:5.42\n",
      "[2023/03/25 22:10:44] Rank:None, Eval at Step [1/13] , FPS 90.1, pred_seen_psnr: 15.14 | \n",
      "[2023/03/25 22:10:46] Rank:None, Eval at Step [13/13] , FPS 81.2, pred_seen_psnr: 15.69 | \n",
      "[2023/03/25 22:10:47] Rank:None, Eval at Step [1/13] , FPS 141.9, quant_seen_psnr: 10.17 | \n",
      "[2023/03/25 22:10:49] Rank:None, Eval at Step [13/13] , FPS 144.1, quant_seen_psnr: 10.9 | \n",
      "Eval at epoch 6 for (640, 1280): pred_seen_psnr: 15.69 | pred_unseen_psnr: 0.0 | quant_seen_psnr: 10.9 | quant_unseen_psnr: 0.0 | \n",
      "[2023/03/25 22:10:51] Rank:None, Epoch[7/10], Step [1/12], lr:4.13e-04 pred_PSNR: 13.49\n",
      "[2023/03/25 22:10:55] Rank:None, Epoch[7/10], Step [12/12], lr:2.63e-04 pred_PSNR: 15.5\n",
      "Time/epoch: \tCurrent:5.38 \tAverage:6.44\n",
      "[2023/03/25 22:10:56] Rank:None, Epoch[8/10], Step [1/12], lr:2.50e-04 pred_PSNR: 19.17\n",
      "[2023/03/25 22:11:00] Rank:None, Epoch[8/10], Step [12/12], lr:1.26e-04 pred_PSNR: 15.81\n",
      "Time/epoch: \tCurrent:5.34 \tAverage:6.31\n",
      "[2023/03/25 22:11:02] Rank:None, Eval at Step [1/13] , FPS 114.9, pred_seen_psnr: 15.34 | \n",
      "[2023/03/25 22:11:04] Rank:None, Eval at Step [13/13] , FPS 116.2, pred_seen_psnr: 15.95 | \n",
      "[2023/03/25 22:11:04] Rank:None, Eval at Step [1/13] , FPS 142.7, quant_seen_psnr: 10.54 | \n",
      "[2023/03/25 22:11:06] Rank:None, Eval at Step [13/13] , FPS 142.9, quant_seen_psnr: 10.81 | \n",
      "Eval at epoch 8 for (640, 1280): pred_seen_psnr: 15.95 | pred_unseen_psnr: 0.0 | quant_seen_psnr: 10.81 | quant_unseen_psnr: 0.0 | \n",
      "[2023/03/25 22:11:08] Rank:None, Epoch[9/10], Step [1/12], lr:1.17e-04 pred_PSNR: 14.82\n",
      "[2023/03/25 22:11:12] Rank:None, Epoch[9/10], Step [12/12], lr:3.53e-05 pred_PSNR: 15.89\n",
      "Time/epoch: \tCurrent:5.44 \tAverage:6.88\n",
      "[2023/03/25 22:11:13] Rank:None, Epoch[10/10], Step [1/12], lr:3.02e-05 pred_PSNR: 18.85\n",
      "[2023/03/25 22:11:17] Rank:None, Epoch[10/10], Step [12/12], lr:2.12e-07 pred_PSNR: 15.98\n",
      "Time/epoch: \tCurrent:5.36 \tAverage:6.73\n",
      "[2023/03/25 22:11:18] Rank:None, Eval at Step [1/13] , FPS 93.4, pred_seen_psnr: 16.17 | \n",
      "[2023/03/25 22:11:20] Rank:None, Eval at Step [13/13] , FPS 113.5, pred_seen_psnr: 16.05 | \n",
      "[2023/03/25 22:11:21] Rank:None, Eval at Step [1/13] , FPS 141.9, quant_seen_psnr: 12.59 | \n",
      "[2023/03/25 22:11:23] Rank:None, Eval at Step [13/13] , FPS 143.2, quant_seen_psnr: 10.62 | \n",
      "After quantization and encoding: \n",
      " bits per parameter: 8.23, bits per pixel: 0.0996\n",
      "Eval at epoch 10 for (640, 1280): pred_seen_psnr: 16.05 | pred_unseen_psnr: 0.0 | quant_seen_psnr: 10.62 | quant_unseen_psnr: 0.0 | \n",
      "results dumped to output/output/sony_hnerv/sony_new_test/1_1_1__Dim64_16_FC9_16_KS0_1_5_RED1.2_low12_blk1_1_e10_b12_quant_M8_E6_lr0.001_cosine_0.1_1_0.1_L2_Size1.5_ENC_convnext_5,4,4,2,2_DEC_pshuffel_5,4,4,2,2__shuffle_datagelu1_1/sony_new_test/1_1_1__Dim64_16_FC9_16_KS0_1_5_RED1.2_low12_blk1_1_e10_b12_quant_M8_E6_lr0.001_cosine_0.1_1_0.1_L2_Size1.5_ENC_convnext_5,4,4,2,2_DEC_pshuffel_5,4,4,2,2__shuffle_datagelu1_1/epoch10.csv\n",
      "Training complete in: 0:01:13.836673\n"
     ]
    }
   ],
   "source": [
    "main() #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose_env",
   "language": "python",
   "name": "pose_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
